import MLJBase
using LinearAlgebra: norm, I, pinv, Bidiagonal, diag, diagm, svd, svdvals
using Flux: onehotbatch, onecold
using MLJ: levels, nrows
using CategoricalArrays: CategoricalArray

mutable struct SimMISVMClassifier <: MLJBase.Deterministic
    C::Float64
    Œ±::Float64
    Œ≤::Float64
    Œº::Float64
    œÅ::Float64
    maxiter::Int64
    tol::Float64
end

function SimMISVMClassifier(; C=1.0, Œ±=1.0, Œ≤=1.0, Œº=.1, œÅ=1.2, maxiter=300, tol=1e-3)
    @assert all(i -> (i > 0), [C, Œ±, Œ≤, Œº, maxiter, tol])
    @assert œÅ > 1.0
    model = SimMISVMClassifier(C, Œ±, Œ≤, Œº, œÅ, maxiter, tol)
end

function MLJBase.fit(model::SimMISVMClassifier, verbosity::Integer, X, y)
    fitresult = X, y, verbosity
    cache = nothing
    report = nothing

    return fitresult, cache, report
end

function MLJBase.predict(model::SimMISVMClassifier, fitresult, Xnew)
    X, y, verbosity = fitresult
    v = init_vars(model, X, Xnew, y)

    if verbosity > 5
        E_res, F_res, Q_res, T_res, R_res, U_res, Z_res = calc_residuals(model, v)
        res = sum([norm(r) for r in (E_res, F_res, Q_res, T_res, R_res, U_res, Z_res)])

        ol = obj_loss(model, v)
        ll = lagrangian_loss(model, v)
        print("Loss: " * string(ol) * "     \t") 
        print("Lagrangian: " * string(ll) * "     \t") 
        println("Residual: " * string(res))
    end

    for i in 1:model.maxiter
        X_update!(model, v)
        S_update!(model, v)
        W_update!(model, v)
        b_update!(model, v)
        E_update!(model, v)
        F_update!(model, v)
        Q_update!(model, v)
        R_update!(model, v)
        T_update!(model, v)
        U_update!(model, v)

        """
        print("X: "); @time X_update!(model, v)
        print("S: "); @time S_update!(model, v)
        print("W: "); @time W_update!(model, v)
        print("b: "); @time b_update!(model, v)
        print("E: "); @time E_update!(model, v)
        print("F: "); @time F_update!(model, v)
        print("Q: "); @time Q_update!(model, v)
        print("R: "); @time R_update!(model, v)
        print("T: "); @time T_update!(model, v)
        print("U: "); @time U_update!(model, v)
        """

        E_res, F_res, Q_res, T_res, R_res, U_res, Z_res = calc_residuals(model, v)

        v.Œõ = v.Œõ + v.Œº * E_res
        v.Œ† = v.Œ† + v.Œº * F_res
        v.Œ£ = v.Œ£ + v.Œº * Q_res
        v.Œò = v.Œò + v.Œº * T_res 
        v.Œ© = v.Œ© + v.Œº * R_res
        v.Œû = v.Œû + v.Œº * U_res
        v.Œî = v.Œî + v.Œº * Z_res

        res = sum([norm(r) for r in (E_res, F_res, Q_res, T_res, R_res, U_res, Z_res)])

        if verbosity > 5
            ol = obj_loss(model, v)
            ll = lagrangian_loss(model, v)
            print("Loss: " * string(ol) * "     \t") 
            print("Lagrangian: " * string(ll) * "     \t") 
            println("Residual: " * string(res))
        end

        if res < model.tol
            break
        end

        v.Œº = model.œÅ * v.Œº
    end

    L = size(v.Y, 2)
    N‚Çó = size(v.T, 2)

    X·µ§ = v.X[:,N‚Çó+1:end]
    X·µ§_cut = [cut .- N‚Çó for cut in v.X_cut[L+1:end]]
    raw_pred = bag_max(v.W'*X·µ§ .+ v.b, X·µ§_cut)

    pred = CategoricalArray(onecold(raw_pred, levels(y)))

    return pred
end

mutable struct simmisvm_vars
    # Original vars
    Z::Array{Float64, 2}
    X::Array{Float64, 2}
    X_cut::Array{UnitRange{Int64}, 1}
    Y::Array{Float64, 2}
    W::Array{Float64, 2}
    b::Array{Float64, 1}
    S::Array{Float64, 2}
    ùìü_Œ©::BitArray{2}

    # Introduced vars
    E::Array{Float64, 2}
    F::Array{Float64, 2}
    Q::Array{Float64, 2}
    R::Array{Float64, 2}
    T::Array{Float64, 2}
    U::Array{Float64, 2}

    # Lagrangian multiplers
    Œõ::Array{Float64, 2}
    Œ†::Array{Float64, 2}
    Œ£::Array{Float64, 2}
    Œò::Array{Float64, 2}
    Œ©::Array{Float64, 2}
    Œû::Array{Float64, 2}
    Œî::Array{Float64, 2}

    # Auxilary vars
    Œº::Float64
    X‚Çó::Array{Float64, 2}
    X‚Çó_cut::Array{UnitRange{Int64}, 1}
    YI::Array{Float64, 2}
    WyX::Array{Float64, 2}
    by::Array{Float64, 2}
    rhs1::Array{Float64, 2}
    rhs2::Array{Float64, 2}
    rhs3::Array{Float64, 2}
end

function init_vars(model::SimMISVMClassifier, _X, _Xnew, _y)
    # Calculate auxilary variables
    P = length(_X) + length(_Xnew)
    L = length(_X)
    _Xall = vcat(_X, _Xnew)

    K = length(levels(_y))
    N‚Çó = sum([nrows(x) for x in _X])
    np = [nrows(x) for x in _Xall]
    X_cut = [sum(np[1:n])-np[n]+1:sum(np[1:n]) for n in 1:length(np)]

    # Build Z by concatenating bags, build Y as onehot matrix
    Z = hcat([MLJBase.matrix(x)' for x in _Xall]...)
    Y = onehotbatch(_y, levels(_y)) .* 2.0 .- 1.0

    # Initialize some original vars
    d, N = size(Z)
    X = randn(d, N)
    S = randn(d, N)
    W = randn(d, K)
    b = randn(K)

    # Build missingness mask and replace NaN values in Z with 0
    ùìü_Œ© = .!ismissing.(Z); Z[ismissing.(Z)] .= 0

    # Build fused lasso term. TODO: later.
    #dv = ones(n)
    #lv = vcat([vcat(-ones(size(x, 1)-1), 0) for x in _Xall]...)[1:n-1]
    #R = Bidiagonal(dv, lv, :L)

    # Introduced vars and associated lagrangian multipleirs
    E = randn(K, L); Œõ = zeros(K, L)
    F = randn(d, N); Œ† = zeros(d, N) 
    Q = randn(K, L); Œ£ = zeros(K, L)
    R = randn(K, L); Œ© = zeros(K, L)
    T = randn(K, N‚Çó); Œû = zeros(K, N‚Çó)
    U = randn(K, N‚Çó); Œò = zeros(K, N‚Çó)

    # Other Lagrangian Multipliers
    Œî = zeros(d, N)
    
    # Auxilary variables
    X‚Çó = randn(d, N‚Çó)
    X‚Çó_cut = X_cut[1:L]
    YI = hcat([repeat(Y[:,i], outer=(1, length(cut))) for (i, cut) in zip(1:N, X‚Çó_cut)]...)
    WyX = randn(K, N‚Çó)
    by = randn(K, N‚Çó)

    rhs1 = zeros(size(X))
    rhs2 = zeros(size(X‚Çó))
    rhs3 = zeros(size(X‚Çó))

    v = simmisvm_vars(Z, X, X_cut, Y, W, b, S, ùìü_Œ©, E, F, Q, R, T, U, Œõ, Œ†, Œ£, Œò, Œ©, Œû, Œî, model.Œº, X‚Çó, X‚Çó_cut, YI, WyX, by, rhs1, rhs2, rhs3)
    calc_X‚Çó_WyX_and_by!(v)

    return v
end

function bag_max(WX, X_cut)
    return hcat([maximum(WX[:, cut], dims=2) for cut in X_cut]...)
end

function calc_X‚Çó_WyX_and_by!(v::simmisvm_vars)
    K, N‚Çó = size(v.T)
    v.X‚Çó = v.X[:,1:N‚Çó]
    WmX = v.W' * v.X‚Çó
    bm = repeat(v.b, outer=(1, size(v.YI, 2)))

    v.WyX = repeat(WmX[v.YI .> 0]', outer=(K, 1))
    v.by = repeat(bm[v.YI .> 0]', outer=(K, 1))
end

function calc_residuals(model::SimMISVMClassifier, v::simmisvm_vars)
    calc_X‚Çó_WyX_and_by!(v)

    E_res = v.E - (v.Y - v.Q + v.R)
    F_res = v.F - v.X
    Q_res = v.Q - bag_max(v.T, v.X‚Çó_cut)
    T_res = v.T - (v.W' * v.X‚Çó .+ v.b)
    R_res = v.R - bag_max(v.U, v.X‚Çó_cut)
    U_res = v.U - (v.WyX + v.by)
    Z_res = v.Z - (v.X + v.S)

    return E_res, F_res, Q_res, T_res, R_res, U_res, Z_res
end

function obj_loss(model::SimMISVMClassifier, v::simmisvm_vars)
    l2reg = 0.5 * norm(v.W, 2)^2
    hinge = model.C * sum(max.(1 .- (bag_max(v.W'*v.X‚Çó .+ v.b, v.X‚Çó_cut) - bag_max(v.WyX + v.by, v.X‚Çó_cut)).*v.Y, 0))
    trace = model.Œ± * sum(svdvals(v.X))
    sparse = model.Œ≤ * norm(v.ùìü_Œ© .* v.S, 1)

    return l2reg + hinge + trace + sparse
end

function lagrangian_loss(model::SimMISVMClassifier, v::simmisvm_vars)
    l2reg = 0.5 * norm(v.W, 2)^2
    hinge = model.C * sum(max.(v.Y .* v.E, 0))
    trace = model.Œ± * sum(svdvals(v.F))
    sparse = model.Œ≤ * norm(v.ùìü_Œ© .* v.S, 1)

    E_res, F_res, Q_res, T_res, R_res, U_res, Z_res = calc_residuals(model, v)

    Ediff = norm(E_res + v.Œõ/v.Œº, 2)^2
    Fdiff = norm(F_res + v.Œ†/v.Œº, 2)^2
    Qdiff = norm(Q_res + v.Œ£/v.Œº, 2)^2
    Tdiff = norm(T_res + v.Œò/v.Œº, 2)^2
    Rdiff = norm(R_res + v.Œ©/v.Œº, 2)^2
    Udiff = norm(U_res + v.Œû/v.Œº, 2)^2
    Zdiff = norm(Z_res + v.Œî/v.Œº, 2)^2

    ùìõ = l2reg + hinge + trace + sparse + 0.5 * v.Œº * (Ediff + Fdiff + Qdiff + Tdiff + Rdiff + Udiff + Zdiff)
end

function X_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    K, N‚Çó = size(v.Y)
    K, NI = size(v.YI)

    Wys = Array{Float64, 2}[]
    lhs = Array{Float64, 2}[]

    for m in 1:K
        Wy = repeat(v.W[:,m], outer=(1,K))
        push!(Wys, Wy)
        push!(lhs, inv(2*I + v.W * v.W' .+ Wy * Wy'))
    end

    v.rhs1 .= v.F .+ v.Œ†./v.Œº .+ v.Z .- v.S .+ v.Œî./v.Œº

    for (p, cut) in enumerate(v.X_cut)
        if p ‚â§ N‚Çó
            p_prime = argmax(v.Y[:,p])
            by = v.b[p_prime]
            v.rhs2[:,cut] .= v.W * (v.T[:,cut] .- v.b .+ v.Œò[:,cut]./v.Œº)
            v.rhs3[:,cut] .= Wys[p_prime] * (v.U[:,cut] .- by .+ v.Œû[:,cut]./v.Œº)
            v.X[:,cut] = lhs[p_prime] * (v.rhs1[:,cut] .+ v.rhs2[:,cut] .+ v.rhs3[:,cut])
        else
            v.X[:,cut] = 0.5*v.rhs1[:,cut]
        end
    end

    calc_X‚Çó_WyX_and_by!(v)
end

function S_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    M = v.Z - v.X + v.Œî/v.Œº
    Mmid = -model.Œ≤/v.Œº .<= M .<= model.Œ≤/v.Œº
    Mgt = M .> model.Œ≤/v.Œº
    Mlt = M .< -model.Œ≤/v.Œº

    v.S = v.ùìü_Œ© .* (M .* .!Mmid - Mgt * (model.Œ≤/v.Œº) + Mlt * (model.Œ≤/v.Œº)) + (.!v.ùìü_Œ©) .* M
end

function W_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    K, NI = size(v.YI)

    lhs1 = sum([(v.T[:,cut] .- v.b + v.Œò[:,cut]/v.Œº) * v.X[:,cut]' for cut in v.X‚Çó_cut])
    rhs1 = sum([v.X[:,cut]*v.X[:,cut]' for cut in v.X‚Çó_cut])

    for m in 1:K
        lhs2 = sum([((v.U[:,cut] - v.by[:,cut] + v.Œû[:,cut]/v.Œº) * v.X[:,cut]') * (v.Y[:,n][m] > 0) for (n, cut) in enumerate(v.X‚Çó_cut)])

        step1 = [v.X[:,cut][:,v.YI[:,cut][m,:] .> 0] for cut in v.X‚Çó_cut]
        rhs2 = sum([x * x' for x in step1])

        v.W[:,m] = (I/v.Œº + rhs1 + K*rhs2)' \ (lhs1[m,:]' + sum(lhs2, dims=1))'
    end
    calc_X‚Çó_WyX_and_by!(v)
end

function b_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    K, NI = size(v.YI)

    numer1 = sum(v.T - v.W'*v.X‚Çó + v.Œò/v.Œº, dims=2)
    numer2 = zeros(size(numer1))
    for m in 1:K
        prime = v.YI[m,:] .> 0
        numer2[m] = sum((v.U - v.WyX + v.Œû/v.Œº)[:,prime])
    end
    numer = numer1 + numer2
    denom = float(NI .+ K * sum(v.YI .> 0.0, dims=2))

    v.b = vec(numer ./ denom)
    calc_X‚Çó_WyX_and_by!(v)
end


function E_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    N = v.Y - v.Q + v.R - v.Œõ/v.Œº
    gt = (v.Y .* N) .> model.C/v.Œº
    mid = 0 .<= (v.Y .* N) .<= model.C/v.Œº

    v.E = N .* .!mid - gt .* v.Y .* (model.C/v.Œº)
end

"""
A singular value thresholding algorithm to solve

min_X Œ±‚ÄñX‚Äñ_* + Œº/2‚ÄñX - A‚Äñ_F^2

where Œ¥ = Œ±/Œº

@article{cai2010singular,
  title={A singular value thresholding algorithm for matrix completion},
  author={Cai, Jian-Feng and Cand{\`e}s, Emmanuel J and Shen, Zuowei},
  journal={SIAM Journal on optimization},
  volume={20},
  number={4},
  pages={1956--1982},
  year={2010},
  publisher={SIAM}
}
"""
function svt(A, Œ¥)
    u, s, v = svd(A)
    return u * diagm(0 => max.(s .- Œ¥, 0)) * v'
end

function F_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    O = v.X - v.Œ†/v.Œº
    v.F = svt(O, model.Œ±/v.Œº)
end

function Q_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    v.Q = 0.5 * (v.Y - v.E + v.R - v.Œõ/v.Œº + bag_max(v.T, v.X‚Çó_cut) - v.Œ£/v.Œº)
end

function R_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    v.R = 0.5 * (v.E - v.Y + v.Q + v.Œõ/v.Œº + bag_max(v.U, v.X‚Çó_cut) - v.Œ©/v.Œº)
end

function T_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    K = size(v.Y, 1)
    Œ¶ = v.W' * v.X‚Çó .+ v.b - v.Œò/v.Œº
    for (i, cut) in enumerate(v.X‚Çó_cut)
        ni = length(cut)
        for m in 1:K
            œï·µ¢‚Çò = Œ¶[m, cut]
            v.T[m,cut] = œï·µ¢‚Çò
            v.T[m,cut[1]+argmax(œï·µ¢‚Çò)-1] = 0.5 * (maximum(œï·µ¢‚Çò) + v.Q[m, i] + v.Œ£[m, i]/v.Œº)
        end
    end
end

function U_update!(model::SimMISVMClassifier, v::simmisvm_vars)
    K = size(v.Y, 1)
    Œ® = v.WyX + v.by - v.Œû/v.Œº
    for (i, cut) in enumerate(v.X‚Çó_cut)
        ni = length(cut)
        for m in 1:K
            œà·µ¢‚Çò = Œ®[m, cut]
            v.U[m,cut] = œà·µ¢‚Çò
            v.U[m,cut[1]+argmax(œà·µ¢‚Çò)-1] = 0.5 * (maximum(œà·µ¢‚Çò) + v.R[m, i] + v.Œ©[m, i]/v.Œº)
        end
    end
end
